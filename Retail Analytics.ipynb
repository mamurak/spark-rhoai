{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a49214-9664-43bd-af63-3b7239ec99d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "from os import getenv\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d427f-19ee-4e93-97d7-9f8e43ee90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_endpoint_url = getenv('AWS_S3_ENDPOINT')\n",
    "s3_access_key_id = getenv('AWS_ACCESS_KEY_ID')\n",
    "s3_secret_access_key = getenv('AWS_SECRET_ACCESS_KEY')\n",
    "s3_bucket_name = getenv('AWS_S3_BUCKET')\n",
    "print(f'S3 endpoint: {s3_endpoint_url}\\n'\n",
    "      f'S3 bucket: {s3_bucket_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03dc53-a29f-4097-b7c2-1cc06eca9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkSession.stop\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName('Retail Analytics')\n",
    "    .config('spark.hadoop.fs.s3a.access.key', s3_access_key_id)\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', s3_secret_access_key)\n",
    "    .config('spark.hadoop.fs.s3a.endpoint', s3_endpoint_url)\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "    .config('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "def quiet_logs(sc):\n",
    "    logger = sc._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger('org').setLevel(logger.Level.ERROR)\n",
    "    logger.LogManager.getLogger('akka').setLevel(logger.Level.ERROR)\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate();\n",
    "quiet_logs(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cb446-5e98-46e5-853e-e734898c6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    # remove missing values\n",
    "    df = df.dropna()\n",
    "    # remove duplicate data\n",
    "    df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_data(spark, file_format, file_path):\n",
    "    s3_path = f's3a://{s3_bucket_name}/{file_path}'\n",
    "    file_reader = spark.read.format(file_format)\n",
    "    if file_format == 'csv':\n",
    "        data = file_reader.load(s3_path, header=True)\n",
    "    else:\n",
    "        data = file_reader.load(s3_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83750f-8bc0-44eb-985b-d0c2af7b8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sales_df = read_data(spark, \"csv\", 'sales_df.csv')\n",
    "stock_df = read_data(spark, \"json\", 'stock.json')\n",
    "supplier_df = read_data(spark, \"json\", 'supplier.json')\n",
    "customer_df = read_data(spark, \"csv\", 'customer.csv')\n",
    "market_df = read_data(spark, \"csv\", 'market.csv')\n",
    "logistic_df = read_data(spark, \"csv\", 'logistic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e3a97-9087-4014-a128-d61c6d1b4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0c911-5d5f-41b3-b288-06863a0a4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fdb090-13ad-4b9c-96d5-c8fed1b4cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694c668-73d1-49e0-97df-6478ff666602",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf985a67-9ec9-45a1-8474-bd0ab76a8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4824489-763a-422f-8871-bc2a94724208",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd957dbf-6610-482e-bde6-a49875b3fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "sales_df = clean_data(sales_df)\n",
    "stock_df = clean_data(stock_df)\n",
    "supplier_df = clean_data(supplier_df)\n",
    "customer_df = clean_data(customer_df)\n",
    "market_df = clean_data(market_df)\n",
    "logistic_df = clean_data(logistic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347ff871-34dd-4bbb-b1fb-a9e4e3d7623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f3e75-ff44-481e-8c41-c4d992eedbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date columns to date type\n",
    "sales_df = sales_df.withColumn(\"date_of_sale\", to_date(col(\"date_of_sale\")))\n",
    "stock_df = stock_df.withColumn(\"date_received\", to_date(col(\"date_received\")))\n",
    "supplier_df = supplier_df.withColumn(\"date_ordered\", to_date(col(\"date_ordered\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2c1f3-d5a0-4e51-9a47-b5caceb997cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize case of string columns\n",
    "sales_df = sales_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"location\", upper(col(\"location\")))\n",
    "supplier_df = supplier_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "customer_df = customer_df.withColumn(\"customer_name\", upper(col(\"customer_name\")))\n",
    "market_df = market_df.withColumn(\"product_name\", upper(col(\"product_name\")))\n",
    "logistic_df = logistic_df.withColumn(\"product_name\", upper(col(\"product_name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fd9c0-2e25-43c0-bc9b-b3732432c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove leading and trailing whitespaces\n",
    "sales_df = sales_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "stock_df = stock_df.withColumn(\"location\", trim(col(\"location\")))\n",
    "supplier_df = supplier_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "customer_df = customer_df.withColumn(\"customer_name\", trim(col(\"customer_name\")))\n",
    "market_df = market_df.withColumn(\"product_name\", trim(col(\"product_name\")))\n",
    "logistic_df = logistic_df.withColumn(\"product_name\", trim(col(\"product_name\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac891b1-01dd-4fb9-bb7f-a7a4eab3c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for invalid values\n",
    "sales_df = sales_df.filter(col(\"product_name\").isNotNull())\n",
    "stock_df = stock_df.filter(col(\"location\").isNotNull())\n",
    "customer_df = customer_df.filter(col(\"gender\").isin(\"male\",\"female\"))\n",
    "market_df = market_df.filter(col(\"product_name\").isNotNull())\n",
    "logistic_df = logistic_df.filter(col(\"product_name\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404755a7-2c17-4ac9-bfe5-4256a8cb5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop extra columns\n",
    "market_df = market_df.drop(\"price\")\n",
    "supplier_df = supplier_df.drop(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd23ca-5766-4793-82af-b4d8f7e7dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all data\n",
    "data_int = (\n",
    "    sales_df.join(stock_df, \"product_name\", \"leftouter\")\n",
    "            .join(supplier_df, \"product_name\", \"leftouter\")\n",
    "            .join(market_df, \"product_name\", \"leftouter\")\n",
    "            .join(logistic_df, \"product_name\", \"leftouter\")\n",
    "            .join(customer_df, \"customer_id\", \"leftouter\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9f167e-b3d8-40f9-8dff-46954d770fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0167b-4b07-4449-8869-789db4158b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_int.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be85126-e72c-4485-9e89-44b3cde387e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned data\n",
    "timestamp = datetime.now().strftime('%y%m%d%H%M')\n",
    "s3_path = f's3a://{s3_bucket_name}/cleaned-{timestamp}.parquet'\n",
    "data_int.write.format(\"parquet\").save(s3_path)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken for Data Cleaning: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb656c3-540c-4601-84c1-2a25c068db60",
   "metadata": {},
   "source": [
    "TODO: update and refactor code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d3c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#DO VARIOUS RETAIL DATA ANALYTICS \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# read cleaned data\n",
    "\n",
    "data = spark.read.format(\"parquet\").load(dataRoot+\"/cleaned/\")\n",
    "\n",
    "#Case when statement to create a new column to indicate whether the product is perishable or not:\n",
    "\n",
    "data = data.withColumn(\"perishable\", when(col(\"shelf_life\") <= 30, \"yes\").otherwise(\"no\"))\n",
    "\n",
    "# You can use the when() and otherwise() functions to create new columns based on certain conditions:\n",
    "\n",
    "data = data.withColumn(\"sales_status\", when(col(\"quantity_sold\") > 50, \"good\").otherwise(\"bad\"))\n",
    "\n",
    "# create a window to perform time series analysis\n",
    "window = Window.partitionBy(\"product_name\").orderBy(\"date_of_sale\")\n",
    "\n",
    "# calculate the rolling average of sales for each product\n",
    "time_series_df = data.withColumn(\"rolling_avg_sales\", avg(\"quantity_sold\").over(window))\n",
    "\n",
    "# use window function for forecasting\n",
    "\n",
    "forecast_df = time_series_df.withColumn(\"prev_sales\", lag(\"rolling_avg_sales\").over(window))\\\n",
    "    .withColumn(\"next_sales\", lead(\"rolling_avg_sales\").over(window))\n",
    "\n",
    "\n",
    "# Calculate the average price of a product, grouped by supplier\n",
    "forecast_df.groupBy(\"sup_id\").agg({\"price\": \"avg\"}).show()\n",
    "\n",
    "\n",
    "# Calculate the total quantity in stock and total sales by supplier\n",
    "forecast_df.groupBy(\"sup_id\").agg({\"quantity_in_stock\": \"sum\", \"price\": \"sum\"}).show()\n",
    "\n",
    "#Calculate the number of perishable v/s non-perishable product per location\n",
    "forecast_df.groupBy(\"perishable\").agg({\"perishable\": \"count\"}).show()\n",
    "\n",
    "\n",
    "#Calculate number of good v/s bad sales status per location\n",
    "forecast_df.groupBy(\"sales_status\").agg({\"sales_status\": \"count\"}).show()\n",
    "\n",
    "# Count the number of sales that contain a 10% off promotion\n",
    "countt = forecast_df.filter(forecast_df[\"contains_promotion\"].contains(\"10% off\")).count()\n",
    "print(countt)\n",
    "# Perform some complex analysis on the DataFrame\n",
    "\n",
    "# Calculate the total sales, quantity sold by product and location\n",
    "total_sales_by_product_location = forecast_df.groupBy(\"product_name\", \"location\").agg(sum(\"price\").alias(\"total_price\"),sum(\"quantity_ordered\").alias(\"total_quantity_sold\"),avg(\"quantity_sold\").alias(\"avg_quantity_sold\")).sort(desc(\"total_price\"))\n",
    "\n",
    "# Group the data by product_name\n",
    "grouped_df = forecast_df.groupBy(\"product_name\")\n",
    "\n",
    "#Sum the quantity_in_stock, quantity_ordered, quantity_sold, and (price * quantity_sold) for each group\n",
    "aggregated_df = grouped_df.agg(sum(\"quantity_in_stock\").alias(\"total_quantity_in_stock\"),avg(\"price\").alias(\"average_price\"),sum(\"quantity_ordered\").alias(\"total_quantity_ordered\"),sum(\"quantity_sold\").alias(\"total_quantity_sold\"),sum(col(\"price\") * col(\"quantity_sold\")).alias(\"total_sales\"),sum(\"prev_sales\").alias(\"total_prev_sales\"),sum(\"next_sales\").alias(\"total_next_sales\"),).sort(desc(\"total_sales\"))\n",
    "\n",
    "#WRITE THE AGGREGATES TO DISK\n",
    "aggregated_df.write.format(\"parquet\").save(dataRoot+\"/app/data.parquet\")\n",
    "total_sales_by_product_location.write.format(\"parquet\").save(dataRoot+\"/app1/data.parquet\")\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time taken on GPU for Data Analysis: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e814c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8578e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
